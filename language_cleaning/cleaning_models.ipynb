{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734eae30-32cd-42d8-a2eb-5b8e3193a6fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models for language cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d362674-b4ea-448a-8787-644715264a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097fdb6-fa28-476a-9cc5-ec392ec25547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 168620 lexemes in Russian\n"
     ]
    }
   ],
   "source": [
    "lexemes = set(open('data/Russian_lexemes.txt', encoding='utf-8').read().split('\\n'))\n",
    "print(f'There are {len(lexemes)} lexemes in Russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc0773-2fc8-48f5-87bc-da7cb13cd937",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Matrix reduction (MR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c02d27-df44-4dbb-8d60-16a6b9aaaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_matrix(matrix, alpha):\n",
    "    non_zero_counts = np.array(np.sum(matrix > 0, axis=0))[0]\n",
    "    quantile = np.quantile(non_zero_counts, alpha)\n",
    "\n",
    "    reduced_indexes = np.where(non_zero_counts <= quantile)[0]\n",
    "    keeped_indexes = np.where(non_zero_counts > quantile)[0]\n",
    "\n",
    "    return [reduced_indexes, keeped_indexes, quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ff2ab-5c19-4f2b-9edb-28ab9362d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/Russian/matrices/TF-IDF.npy', allow_pickle=True)\n",
    "W, words = data[0], data[1]\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e09cd-2185-4468-b6d2-be4a235db235",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = (0.05, 0.25, 0.4, 0.5, 0.55, 0.6, 0.75, 0.8, 0.85 ,0.9, 0.95, 0.975)\n",
    "\n",
    "f = open('language_cleaning/MR/MR_report.txt', 'w')\n",
    "\n",
    "for alpha in alphas:\n",
    "    f.write(f'\\nQuantile of {alpha} level: {quantile}\\n')\n",
    "    os.makedirs(f'language_cleaning/MR/{alpha}')\n",
    "    reduced_indexes, keeped_indexes, quantile = reduce_matrix(W, alpha)\n",
    "\n",
    "    for fname, indexes in zip(('words', 'rubbish'), (keeped_indexes, reduced_indexes)):\n",
    "        sample = words[indexes]\n",
    "\n",
    "        open(f'language_cleaning/MR/{alpha}/{fname}.txt', 'w').write('\\n'.join(sample))\n",
    "\n",
    "        sample_share = round(100 * len(sample) / len(words), 1)\n",
    "        lexemes_in_sample = len(lexemes & set(sample))\n",
    "        lexemes_share = round(100 * lexemes_in_sample / len(sample), 1)\n",
    "\n",
    "        # sample size (its share of number of words), number of lexemes (its share of number of words)\n",
    "        f.write(\n",
    "            f'Length of {fname} list: {len(sample)} ({sample_share}%), {lexemes_in_sample} lexemes ({lexemes_share}%)\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df8058-6c99-4c59-a18e-5e990611e64f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Median neighbour popularity (MNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9acefc-31c9-4583-9fd8-cc95d013857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_words_by_neighbours_median(lang, gamma, words):\n",
    "    fnames = np.array(glob.glob(f'data/{lang}/neighbours/*'))\n",
    "    start_indicies = np.array([int(re.search('[0123456789]+', fname.split('/')[-1]).group()) for fname in fnames])\n",
    "    fnames = fnames[np.argsort(start_indicies)]\n",
    "    neighbours_data = [np.load(fname, allow_pickle=True) for fname in fnames]\n",
    "\n",
    "    stat, word_index = [[], 0]\n",
    "    for sample in neighbours_data:\n",
    "        for word_data in sample:\n",
    "            assert word_index == word_data[0]\n",
    "            stat.append(word_data[1])\n",
    "            word_index += 1\n",
    "    stat = np.array(stat)\n",
    "\n",
    "    return [\n",
    "        words[np.argsort(stat[:,0])][:int(len(words) * gamma)],\n",
    "        words[np.argsort(stat[:,0])][int(len(words) * gamma):]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52428df-a16d-4ba0-b53e-2090b1213069",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.load('data/Russian/matrices/TF-IDF.npy', allow_pickle=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9b304-b72b-4634-8f62-00de5b35fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)\n",
    "\n",
    "f = open('language_cleaning/MNP/MNP_report.txt', 'w')\n",
    "\n",
    "for gamma in gammas:\n",
    "    f.write(f'\\ngamma = {gamma}\\n')\n",
    "    os.makedirs(f'language_cleaning/MNP/{gamma}')\n",
    "    garbage_words, good_words = select_words_by_neighbours_median('Russian', gamma, words)\n",
    "\n",
    "    for fname, sample in zip(('rubbish', 'words'), (garbage_words, good_words)):    \n",
    "        open(\n",
    "            f'language_cleaning/MNP/{gamma}/{fname}.txt', 'w'\n",
    "        ).write('\\n'.join(sorted(sample)))\n",
    "\n",
    "        sample_share = round(100 * len(sample) / len(words), 1)\n",
    "        lexemes_in_sample = len(lexemes & set(sample))\n",
    "        lexemes_share = round(100 * lexemes_in_sample / len(sample), 1)\n",
    "\n",
    "        # sample size (its share of number of words), number of lexemes (its share of number of words)\n",
    "        f.write(\n",
    "            f'Length of {fname} list: {len(sample)} ({sample_share}%), {lexemes_in_sample} lexemes ({lexemes_share}%)\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e826d-cfe3-49d4-9479-749fdd8b809c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f288236-90e7-448d-aceb-0f4aea517898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 122667 words and 53588 lexemes in dict\n"
     ]
    }
   ],
   "source": [
    "embedding_dict = np.load('data/Russian_dict_SVD_32.npy', allow_pickle=True).item()\n",
    "words = np.array(list(embedding_dict.keys()))\n",
    "embeddings = np.vstack([embedding_dict[word] for word in words])\n",
    "\n",
    "print(f'There are {len(words)} words and {len(lexemes & set(words))} lexemes in dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fa5ed-a6a7-4193-8e2e-e927b77ec27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = (\n",
    "    0.001, 0.002, 0.003, 0.004, 0.005, 0.01, 0.02,\n",
    "    0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1\n",
    ")\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    dbscan = DBSCAN(eps=epsilon, n_jobs=-1).fit(embeddings)\n",
    "    labels = dbscan.labels_ - np.min(dbscan.labels_)\n",
    "\n",
    "    os.makedirs(f'language_cleaning/DBSCAN/{epsilon}')\n",
    "    for label in np.unique(labels):\n",
    "        open(\n",
    "            f'language_cleaning/DBSCAN/{epsilon}/{label}.txt', 'w'\n",
    "        ).write('\\n'.join(words[labels == label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c805fbf-f482-4e86-8449-9dbe691c46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = sorted(os.listdir('language_cleaning/DBSCAN'))\n",
    "\n",
    "f = open('language_cleaning/DBSCAN/DBSCAN_report.txt', 'w')\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    f.write(f'\\nepsilon = {epsilon}\\n')\n",
    "    labels = sorted([\n",
    "        label[:-4] for label in os.listdir(f'language_cleaning/DBSCAN/{epsilon}')\n",
    "    ])\n",
    "\n",
    "    for label in labels:\n",
    "        sample = open(f'language_cleaning/DBSCAN/{epsilon}/{label}.txt').read().split('\\n')\n",
    "\n",
    "        sample_share = round(100 * len(sample) / embeddings.shape[0])\n",
    "        lexemes_in_sample = len(lexemes & set(sample))\n",
    "        lexemes_share = round(100 * lexemes_in_sample / len(sample))\n",
    "\n",
    "        # sample size (its share of number of words), number of lexemes (its share of number of words)\n",
    "        f.write(\n",
    "            f'{label}: {len(sample)} words ({sample_share}%), {lexemes_in_sample} lexemes ({lexemes_share}%)\\n'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d2afc-1031-4c48-a0cf-7117098464d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Integral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734d733-b7d1-40a8-9f17-7614c906fb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "matrix & neighs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.98      0.75     70120\n",
      "           1       0.84      0.15      0.26     50803\n",
      "\n",
      "    accuracy                           0.63    120923\n",
      "   macro avg       0.73      0.56      0.51    120923\n",
      "weighted avg       0.71      0.63      0.55    120923\n",
      "\n",
      "\n",
      "\n",
      "matrix & dbscan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.95      0.72     70120\n",
      "           1       0.47      0.06      0.11     50803\n",
      "\n",
      "    accuracy                           0.58    120923\n",
      "   macro avg       0.53      0.51      0.42    120923\n",
      "weighted avg       0.54      0.58      0.47    120923\n",
      "\n",
      "\n",
      "\n",
      "neighs & dbscan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.73     70120\n",
      "           1       0.56      0.00      0.01     50803\n",
      "\n",
      "    accuracy                           0.58    120923\n",
      "   macro avg       0.57      0.50      0.37    120923\n",
      "weighted avg       0.57      0.58      0.43    120923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = np.load('languages/Russian/matrices/TF-IDF.npy', allow_pickle=True)[1]\n",
    "\n",
    "rubbish = [\n",
    "    open(\n",
    "        f'language_cleaning/{rubbish_path}.txt'\n",
    "    ).read().split('\\n') for rubbish_path in (\n",
    "        'MR/0.6/rubbish', 'MNP/0.1/rubbish', 'DBSCAN/0.002/0'\n",
    "    )\n",
    "]\n",
    "\n",
    "models = ('matrix', 'neighs', 'dbscan')\n",
    "for i in range(len(models)):\n",
    "    for j in range(i + 1, len(models)):\n",
    "        curr_rubbish = set(rubbish[i]) & set(rubbish[j])\n",
    "        y_true = [int(word not in lexemes) for word in words]\n",
    "        y_pred = [int(word in curr_rubbish) for word in words]\n",
    "\n",
    "        print('\\n\\n' + models[i] + ' & ' + models[j])\n",
    "        print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
