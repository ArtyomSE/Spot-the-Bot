{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd95b877-35f4-442e-9dbd-57c98ca8fed8",
   "metadata": {},
   "source": [
    "# Pipeline for language cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864481-dcc8-4cff-b090-27efd99bd3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ddece",
   "metadata": {},
   "source": [
    "### Deleting translations from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_translations_from_corpus(lang):\n",
    "    data = pd.read_excel('language_cleaning/translations_list.xlsx', header=0, index_col=None)\n",
    "    fnames_to_keep = data[lang].values.astype(str)\n",
    "    fnames_to_keep = fnames_to_keep[fnames_to_keep != 'nan'].astype(float).astype(int).astype(str)\n",
    "\n",
    "    fnames = [fname[:-4] for fname in os.listdir(f'data/{lang}/{lang}_corpus_processed')]\n",
    "    assert fnames_to_keep.shape[0] == len(set(fnames).intersection(set(fnames_to_keep)))\n",
    "\n",
    "    os.makedirs(f'data/{lang}/{lang}_corpus_processed_clean')\n",
    "\n",
    "    for fname_to_keep in fnames_to_keep:\n",
    "        shutil.copy2(\n",
    "            f'data/{lang}/{lang}_corpus_processed/{fname_to_keep}.txt',\n",
    "            f'data/{lang}/{lang}_corpus_processed_clean/{fname_to_keep}.txt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3916659",
   "metadata": {},
   "source": [
    "### TF-IDF matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def make_tf_idf_matrix(lang):\n",
    "    open(f'data/{lang}/{lang}_corpus_processed_clean.txt', 'w').write(\n",
    "        '\\n'.join([\n",
    "            open(\n",
    "                f'data/{lang}/{lang}_corpus_processed_clean/{fname}'\n",
    "            ).read() for fname in os.listdir(\n",
    "                f'data/{lang}/{lang}_corpus_processed_clean'\n",
    "            )\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    corpus = open(f'data/{lang}/{lang}_corpus_processed_clean.txt', 'r')\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', min_df=3, token_pattern='(?u)\\\\b\\\\w+\\\\b', lowercase=True)\n",
    "    data_vectorized = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    np.save(f'data/{lang}/matrices/TF-IDF.npy', np.asarray(\n",
    "        (data_vectorized, vectorizer.get_feature_names_out()), dtype=object\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5dbf6a-49fa-4a54-ac48-5ae413475c5a",
   "metadata": {},
   "source": [
    "### MNP rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_count(args):\n",
    "    word_index, n_text, x, y = args\n",
    "\n",
    "    text_inds = x[y == word_index]\n",
    "    neig = y[np.isin(x, text_inds) & (y != word_index)]\n",
    "    stat = np.median(n_text[neig]), np.mean(n_text[neig])\n",
    "\n",
    "    return word_index, stat\n",
    "\n",
    "def calc_neighbours_median_popularity(lang):\n",
    "    tf_idf_matrix = np.load(f'data/{lang}/matrices/TF-IDF.npy', allow_pickle=True)[0]\n",
    "    n_text = np.array(np.sum(tf_idf_matrix, axis=0))[0]\n",
    "\n",
    "    x, y = tf_idf_matrix.nonzero()\n",
    "    checking = sorted(np.unique(y))\n",
    "\n",
    "    step = 5000\n",
    "    for start in range(0, len(checking), step):\n",
    "        finish = min(len(checking), start + step)\n",
    "        if f'{start}_{finish}.npy' in os.listdir(f'data/{lang}/neighbours'): continue\n",
    "\n",
    "        pool = Pool(processes=8)\n",
    "        result = pool.map(stat_count, list(zip(\n",
    "            checking[start:finish], [n_text for _ in range(finish - start)],\n",
    "            [x for _ in range(finish - start)], [y for _ in range(finish - start)]\n",
    "            ))\n",
    "        )\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        np.save(f'data/{lang}/neighbours/{start}_{finish}.npy', np.asarray(result, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795e734-bf4c-49ea-b078-d4b28fa9e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_words_by_neighbours_median(lang):\n",
    "    fnames = np.array(glob.glob(f'data/{lang}/neighbours/*'))\n",
    "    start_indicies = np.array([int(re.search('[0123456789]+', fname.split('/')[-1]).group()) for fname in fnames])\n",
    "    fnames = fnames[np.argsort(start_indicies)]\n",
    "    neighbours_data = [np.load(fname, allow_pickle=True) for fname in fnames]\n",
    "\n",
    "    stat, word_index = [[], 0]\n",
    "    for sample in neighbours_data:\n",
    "        for word_data in sample:\n",
    "            assert word_index == word_data[0]\n",
    "            stat.append(word_data[1])\n",
    "            word_index += 1\n",
    "    stat = np.array(stat)\n",
    "\n",
    "    words = np.load(f'data/{lang}/matrices/TF-IDF.npy', allow_pickle=True)[1]\n",
    "    return words[np.argsort(stat[:,0])][:int(len(words) * 0.1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46d080-27f6-49f0-a602-4dc694691fb4",
   "metadata": {},
   "source": [
    "### MR rubbish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f49d26-cf46-4f0e-aad2-bae409f026f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_matrix(lang, alpha=0.6):\n",
    "    data = np.load(f'data/{lang}/matrices/TF-IDF.npy', allow_pickle=True)\n",
    "    tf_idf_matrix, words = data[0], data[1]\n",
    "\n",
    "    non_zero_counts = np.array(np.sum(tf_idf_matrix > 0, axis=0))[0]\n",
    "    quantile = np.quantile(non_zero_counts, alpha)\n",
    "    # print(f'\\nQuantile of {alpha} level: {quantile}')\n",
    "\n",
    "    return words[np.where(non_zero_counts <= quantile)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3965e",
   "metadata": {},
   "source": [
    "### SVD computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b16a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def compute_SVD(lang, k_max=1000):\n",
    "    tf_idf_matrix = np.load(f'data/{lang}/matrices/TF-IDF_clean.npy', allow_pickle=True)[0]\n",
    "\n",
    "    # Computing SVD:\n",
    "    k = min(k_max, min(tf_idf_matrix.shape) - 1) # 0 < k < min(A.shape)\n",
    "    u, sigma, vt = svds(tf_idf_matrix, k)\n",
    "\n",
    "    # Sorting singular values in descending order (function doesn't garantee it):\n",
    "    descending_order_of_inds = np.flip(np.argsort(sigma))\n",
    "    u = u[:,descending_order_of_inds]\n",
    "    vt = vt[descending_order_of_inds]\n",
    "    sigma = sigma[descending_order_of_inds]\n",
    "\n",
    "    # Checking that sizes are correct:\n",
    "    assert sigma.shape == (k,)\n",
    "    assert vt.shape == (k, tf_idf_matrix.shape[1])\n",
    "    assert u.shape == (tf_idf_matrix.shape[0], k)\n",
    "\n",
    "    # Saving the matrices:\n",
    "    matrices_names = ('sigma_vt', 'sigma', 'u', 'vt')\n",
    "    matrices = (np.dot(np.diag(sigma), vt).T, sigma, u, vt)\n",
    "\n",
    "    for matrix_name, matrix in zip(matrices_names, matrices):\n",
    "        with open(f'data/{lang}/matrices/{k}_{matrix_name}.npy', 'wb') as f:\n",
    "            np.save(f, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36cd5d",
   "metadata": {},
   "source": [
    "### SVD embeddings computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3453c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SVD_dictionary(lang):\n",
    "    words = np.load(f'data/{lang}/matrices/TF-IDF_clean.npy', allow_pickle=True)[1]\n",
    "    sigma_vt = np.load(glob.glob(f'data/{lang}/matrices/*_sigma_vt.npy')[0], allow_pickle=True)\n",
    "\n",
    "    dictionary = dict([[word, vector] for word, vector in zip(words, sigma_vt)])\n",
    "\n",
    "    np.save(f'data/{lang}/{lang}_dict_SVD_{sigma_vt.shape[1]}.npy', dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c2e3bc-0a3a-4db0-96ff-45748e11d187",
   "metadata": {},
   "source": [
    "### CBoW embeddings computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37cb06-3cf7-4366-a1cf-0ee1f7a78187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CBoW_dictionary(lang, k=100):\n",
    "    fin = io.open(\n",
    "        f'data/{lang}/{lang}_corpus_processed_clean', 'r',\n",
    "        encoding='utf-8', newline='\\n', errors='ignore'\n",
    "    )\n",
    "    documents = [line.split() for line in fin]\n",
    "    \n",
    "    model = Word2Vec(sentences=documents, vector_size=k, min_count=1)\n",
    "    dictionary = {key : model.wv[key] for key in model.wv.key_to_index}\n",
    "\n",
    "    np.save(f'data/{lang}/{lang}_dict_CBoW_{k}.npy', dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa180227-39f9-4f1b-98ae-b15e84a5cfbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Language cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_language(lang):\n",
    "    # Deleting translations from corpus:\n",
    "    delete_translations_from_corpus(lang=lang)\n",
    "\n",
    "    # Computing TF-IDF matrix:\n",
    "    os.makedirs(f'data/{lang}/matrices')\n",
    "    make_tf_idf_matrix(lang=lang)\n",
    "\n",
    "    # Computing garbage words with 2 methods:\n",
    "    os.makedirs(f'data/{lang}/neighbours')\n",
    "    calc_neighbours_median_popularity(lang=lang)\n",
    "    garbage_words_neighs_median = select_words_by_neighbours_median(lang=lang)\n",
    "    garbage_words_matrix_reduction = reduce_matrix(lang=lang)\n",
    "\n",
    "    garbage_words = set(garbage_words_neighs_median).union(garbage_words_matrix_reduction)\n",
    "    with open(f'data/{lang}/{lang}_garbage_words.txt', 'w') as fout:\n",
    "        fout.write('\\n'.join(sorted(garbage_words)))\n",
    "\n",
    "    # Writing good words:\n",
    "    data = np.load(f'data/{lang}/matrices/TF-IDF.npy', allow_pickle=True)\n",
    "    tf_idf_matrix, words = data[0], data[1]\n",
    "\n",
    "    clean_words = sorted(list(set(words).difference(garbage_words)))\n",
    "    with open(f'data/{lang}/{lang}_good_words.txt', 'w') as fout:\n",
    "        fout.write('\\n'.join(clean_words))\n",
    "\n",
    "    # Computing clean TF-IDF matrix:\n",
    "    word_list = list(words)\n",
    "    kept_indicies = np.array([word_list.index(clean_word) for clean_word in clean_words])\n",
    "    for i in range(len(kept_indicies) - 1):\n",
    "        assert kept_indicies[i] < kept_indicies[i + 1]\n",
    "\n",
    "    np.save(f'data/{lang}/matrices/TF-IDF_clean.npy', np.asarray(\n",
    "        (tf_idf_matrix[:, kept_indicies], words[kept_indicies]), dtype=object\n",
    "    ))\n",
    "\n",
    "    # Computing SVD and creating dictionaries:\n",
    "    compute_SVD(lang=lang)\n",
    "    create_SVD_dictionary(lang=lang)\n",
    "    create_CBoW_dictionary(lang=lang, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_language('Russian')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
